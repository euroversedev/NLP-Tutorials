{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제목 : BERT 실습 - 마스크드 언어 모델 (Masked Language Model) 실습  \n",
    "학습일 : 2022-07-11   \n",
    "작성자 : [송유럽(Euro)](https://www.euroverse.dev/)  \n",
    "관련 링크 : [딥러닝을 이용한 자연어처리 입문](https://wikidocs.net/153992)\n",
    "  \n",
    "--- 설명 ---  \n",
    "\n",
    "--- 더 공부할 내용 ---  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForMaskedLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 5.73kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 372kB/s] \n",
      "Downloading: 100%|██████████| 455k/455k [00:01<00:00, 457kB/s]  \n"
     ]
    }
   ],
   "source": [
    "model = TFBertForMaskedLM.from_pretrained('bert-large-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 101 4715 2003 1037 2428 4569  103 1012  102]], shape=(1, 9), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('Soccer is a really fun [MASK].', return_tensors='tf')\n",
    "print(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0 0 0 0 0 0 0 0 0]], shape=(1, 9), dtype=int32)\n",
      "tf.Tensor([[1 1 1 1 1 1 1 1 1]], shape=(1, 9), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(inputs['token_type_ids'])\n",
    "print(inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7621118426322937,\n",
       "  'token': 4368,\n",
       "  'token_str': 'sport',\n",
       "  'sequence': 'soccer is a really fun sport.'},\n",
       " {'score': 0.20341995358467102,\n",
       "  'token': 2208,\n",
       "  'token_str': 'game',\n",
       "  'sequence': 'soccer is a really fun game.'},\n",
       " {'score': 0.012208560481667519,\n",
       "  'token': 2518,\n",
       "  'token_str': 'thing',\n",
       "  'sequence': 'soccer is a really fun thing.'},\n",
       " {'score': 0.0018630244303494692,\n",
       "  'token': 4023,\n",
       "  'token_str': 'activity',\n",
       "  'sequence': 'soccer is a really fun activity.'},\n",
       " {'score': 0.0013354836264625192,\n",
       "  'token': 2492,\n",
       "  'token_str': 'field',\n",
       "  'sequence': 'soccer is a really fun field.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import FillMaskPipeline\n",
    "pip = FillMaskPipeline(model=model, tokenizer=tokenizer)\n",
    "pip('Soccer is a really fun [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.34832027554512024,\n",
       "  'token': 3835,\n",
       "  'token_str': 'nice',\n",
       "  'sequence': 'euro is very nice guy.'},\n",
       " {'score': 0.06406177580356598,\n",
       "  'token': 2204,\n",
       "  'token_str': 'good',\n",
       "  'sequence': 'euro is very good guy.'},\n",
       " {'score': 0.04622715339064598,\n",
       "  'token': 6047,\n",
       "  'token_str': 'smart',\n",
       "  'sequence': 'euro is very smart guy.'},\n",
       " {'score': 0.02771160565316677,\n",
       "  'token': 4658,\n",
       "  'token_str': 'cool',\n",
       "  'sequence': 'euro is very cool guy.'},\n",
       " {'score': 0.01855863630771637,\n",
       "  'token': 4086,\n",
       "  'token_str': 'sweet',\n",
       "  'sequence': 'euro is very sweet guy.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pip('Euro is very [MASK] guy.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 보니 매우 정확한 모델인 듯 하다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff4f85d6e04298634172ac5d8264e7e9b556b95639fe52ebb9425c4d4cba0c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
